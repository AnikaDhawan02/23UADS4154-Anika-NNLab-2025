Objective:
WAP to implement a multi-layer perceptron (MLP) network with one hidden layer using numpy in Python. Demonstrate that it can learn the XOR Boolean function.  

Description:
This MLP model uses the step function as its activation function for both hidden and output layers. The step function is a threshold-based function that outputs 0 or 1 depending on whether the input is negative or non-negative. It is often used for binary classification but is not ideal for gradient-based training because its derivative is zero almost everywhere, preventing effective weight updates during backpropagation.

The network consists of:
1. An input layer with two neurons (corresponding to the XOR inputs).
2.  A single hidden layer with two neurons and step activation.
3. An output layer with one neuron, also using the step activation.

The XOR dataset is provided as input, containing all combinations of binary values. Training involves multiple epochs where the network attempts to minimize the difference between predicted and expected outputs. However, due to the limitations of the step function, the model might not converge to the correct XOR mapping.

Code Description:
1. The step function and its derivative are defined.
2. The MLP class includes methods for initialization, forward propagation, backward propagation, and training.
3. Weights and biases are initialized randomly to break symmetry during learning.
4. Predictions are made post-training and presented in a tabular format for clarity.
5. The training loop iterates over a fixed number of epochs, though improvements in predictions may be limited due to zero gradients.





Output (Table Format):
Input 1   Input 2    Expected Output    Predicted Output
     0        0               0                0
     0        1               1                1
     1        0               1                1
     1        1               0                1

(Note: Predicted output may vary due to random weight initialization.)

Performance:
Due to the step function's zero derivative, performance is expected to be suboptimal, and the XOR function may not be learned effectively. The output may not match the expected XOR truth table accurately, highlighting the need for alternative activation functions.

My Comments:
While this code demonstrates the concept of a multi-layer perceptron, using a step function is not practical for gradient-based training due to non-differentiability. For effective learning, consider using activation functions like sigmoid or ReLU, which provide non-zero gradients, enabling the network to learn complex patterns like XOR.

